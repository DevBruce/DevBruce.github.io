---
title: "[ML] Deep Learning Basic"
excerpt: 
last_modified_at: 2019-03-29

categories:
  - MachineLearning

tags:
  - machine_learning
  - deep_learning
  - neural_network
  - layer
  - weight
  - hypothesis
  - logistic_hypothesis
  - cost_function
  - logistic_cost_function

---

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	  tex2jax: {
	    inlineMath: [ ['$','$'], ['\\(','\\)'] ]
	  }
	});
</script>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>

# Deep Learning Basic

## Layer

### Input Layer

- 값을 입력받아 은닉계층으로 전달하는 계층

- Input Layer 의 노드 갯수는 입력값의 갯수와 동일하다.  

- Logistic Function(Activation Function) 연산없이 Hidden Layer 에 전달한다.

> ex) 30 * 30 픽셀을 가진 이미지를 입력받을 경우 입력계층의 노드갯수는 900개가 된다.  
> (흑백픽셀은 한개당 0~255의 값을 갖는다.)

<br>

### Hidden Layer

- Input Layer 에서 전달받아 Hidden Layer 또는 Output Layer 로 전달한다.  
마지막 Hideen Layer 에서는 주로 결과값이 0 ~ 1 사이의 값을 반환하는  
Logistic Function 을 사용하여 Output Layer 로 전달한다.

- Hidden Layer 의 갯수와 노드 갯수는 임의로 정할 수 있다.  
Layer 와 노드가 많아질수록 Deep 하다고 표현된다.  
정확도를 높이기 위해 각각의 갯수를 경험적으로 조절할 필요도 있다.

<br>

### Output Layer

- Hidden Layer 에서 전달받아 최종 결과값을 출력한다.

- Output Layer 의 노드갯수는 구분하고자 하는 클래스의 갯수라고 할 수 있다.  

- Softmax 를 통해 각 출력값을 확률로 변환하여 출력값의 합이 1 이 되는 형태가 선호된다.  
이를 위해 마지막 Hidden Layer 의 Logistic Function 을 Sigmoid 같은 형태를 사용한다.

> ex) 1, 2, 3, 4, 5 가 쓰여진 이미지를 구분하는 신경망의 경우  
> Output Layer 의 노드갯수는 5개가 되는 형태이다.  
> 즉 첫번째 노드의 값이 가장크면 1 로, 두번째 노드의 값이 가장크면 2 로 구분하게 되는 것

<br><br>

## Weight

- 입력받은 값을 다음 계층으로 값을 전달할 때의 비율값

- 지나치게 큰 Weight 는 Logistic Function(Activation Function)을 포화시킬 수 있다.

- Weight 가 0 이 되면 학습능력을 상실한다.

<br><br>

## Hypothesis

Hypothesis 는 다음과 같이 표현된다.

> - W: Weight
> - b: Bias

- Theory: $ H(x) = Wx + b $

- Implementation Code For TensorFlow: $ H(X) = XW + b $

```python
# Python Implementation Code For TensorFlow
hypothesis = (X * W) + b

# 위와 동일한 hypothesis
hypothesis = tf.matmul(X, W) + b
```

<br><br>

## Logistic Hypothesis

Hypothesis 에 Logistic Function (Activation Function) 연산을 수행한 것을  
Logistic Hypothesis 라고 한다.  

Logistic Function (Activation Function)의 예시는 다음과 같다.  

- Sigmoid: $ \frac{1}{1 + e^{-x}} $
  - 연산결과의 범위가 $ 0 < \frac{1}{1 + e^{-H(x)}} < 1 $ 와 같이 된다.
  - Gradient Vanishing 문제가 발생한다.

<br>

- ReLU: $ \cases {f(x)=0\ (x < 0) \cr f(x)=x\ (x \geq 0)} $
  - 주로 사용되는 Logistic Function 이다.
  - 단순하고 빠르다.
  - Gradient Vanishing 문제가 해결된다.
  - 음수는 모두 0으로 처리하는 단점이 존재한다.
  
> Logistic Function 으로 ReLU 를 사용하더라도 마지막 Layer 에서는  
> 0 과 1 사이의 값을 출력하는 Sigmoid 와 같은 Logistic Function 을 사용한다.

<br><br>

## Cost Function (MSE)

- Regression 에서 주로 사용
- MSE (Mean Square Error): 평균제곱오차

<br>

$$ cost(W, b) = \frac{1}{m} \sum_{i=1}^m (H(x^{(i)}) - y^{(i)})^2 $$

- $ H(x^{(i)}) $: Prediction Value
- $ y^{(i)} $: True Value

```python
# Python Implementation Code For TensorFlow
cost = tf.reduce_mean(tf.square(hypothesis - Y))
```

> Cost Function 을 최소화 시키는 것이 training 의 목적이라고 할 수 있다.

<br><br>

## Logistic Cost Function

**Cost Function For Logistic Hypothesis**

- Classification 에서 주로 사용  
- Cross-entropy Cost Function 과 결과적으로 동일

<br>

H(x) 가 Logistic Hypothesis 일 경우  
Cost Function (MSE) 의 모양이 완만한 Convex 형태가 아니게 된다.  
이런 경우 Gradient Descent Algorithtm 을 사용했을 때  
최저점을 Global Minimum 이 아닌 Local Minimum 으로 인식할 수 있다.  

따라서 $ (H(x^{(i)}) - y^{(i)})^2 $ 을 변형시킬 것이다.  

이럴경우 Cost Function 의 형태는 아래와 같이 된다.  

<br>

$$ cost(W, b) = \frac{1}{m}\sum New(H(x),y) $$

<br>

$ New(H(x),y) $ 는 그래프의 모양을 부드럽게 만들기 위해  
$log$ 를 취하는 방식으로 $ (H(x^{(i)}) - y^{(i)})^2 $ 를 변형한 것이다.  

<br>

$ New(H(x),y) = \cases {-log(H(x))\quad(y=1) \cr -log(1-H(x))\quad(y=0)} $

<br>

$ y=1 $, $ y=0 $ 두가지 케이스로 나누어진 식은 다음과 같이 한줄로 변형이 가능하다.

<br>

$ New(H(x),y) = -y\log(H(x)) + (1 -y)\log(1 - H(x)) $

<br>

최종적인 New Cost Function 은 다음과 같은 형태가 된다.

$$ cost(W, b) = -\frac{1}{m}\sum ( y\log(H(x)) + (1 -y)\log(1 - H(x)) ) $$

```python
# Python Implemenation Code For TensorFlow
hypothesis_sigmoid = tf.sigmoid(hypothesis)
cost = -tf.reduce_mean(Y * tf.log(hypothesis_sigmoid) + (1-Y) * tf.log(1-hypothesis_sigmoid))
```

<br>

> ### Cross-entropy Cost Function
> 
> Cross-entropy Function 의 **평균**을 Cross-entropy Cost Function 이라고 한다.
>
> ```python
> # Python Implemenation Code For TensorFlow
> hypothesis_softmax = tf.nn.softmax(hypothesis)
> cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis_softmax), axis=1)
> ```
> 
> <br>
> 
> TensorFlow 에 구현되어있는 `softmax_cross_entropy_with_logits`
> 
> ```python
> # 아래의 logits 은 softmax 가 적용되지 않은 hypothesis 가 들어갈 수 있다.
> cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
> cost = tf.reduce_mean(cross_entropy)
> ```

<br><br>

# Reference

- [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
